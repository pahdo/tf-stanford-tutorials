{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Don't predict, count!\n",
    "\n",
    "Should only attempt after after class about word2vec (1/25).\n",
    "\n",
    "Classical word-counting models still have some merit. They are easy to build and train and have surprisingly good results.\n",
    "\n",
    "In this problem, you will build a context-counting model on the dataset of your choice. You can download the text8 dataset (downloaded to data/text8). The data is already processed, you can get tokens just by splitting the text at the spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to building your context-counting model:\n",
    "1. Read in your data, build the vocabulary.\n",
    "2. Build co-occurrence matrix.\n",
    "3. Use SVD to reduce the dimensionality of your co-occurrence matrix to your embedding size. You should use tf.svd for this.\n",
    "\n",
    "The resulting co-occurrence matrix is your embedding matrix, each row corresponds to the vector representation of one vector.\n",
    "\n",
    "I advise you to keep your vocabulary under 10,000 words. You can choose the 9,999 most frequent tokens and replace any other tokens with UNK. The embedding size can be anywhere from 50 to 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "[0, 0, 12, 6, 195, 2, 0, 46, 59, 156, 128, 742, 477, 0, 134, 1, 0, 2, 1, 103, 855, 3, 1, 0, 0]\n",
      "(1000, 1000)\n",
      "(1000, 1000)\n",
      "Processing data and constructing matrix took: 0:01:06.081463\n"
     ]
    }
   ],
   "source": [
    "from six.moves import urllib\n",
    "import os\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters for the model\n",
    "VOCAB_SIZE = 1000\n",
    "EMBED_SIZE = 150 # Between 50 and 300 is good.\n",
    "SKIP_WINDOW = 1\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_FOLDER = 'data/'\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    file_path = DATA_FOLDER + file_name\n",
    "    if (os.path.exists(file_path)):\n",
    "        print(\"Dataset ready\")\n",
    "        return file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if (file_stat.st_size == expected_bytes):\n",
    "        print(\"Successfully downloaded the file\", file_name)\n",
    "    else:\n",
    "        raise Exception(\"File \" + file_name + \" might be corrupted. You should try downloading it with a browser.\")\n",
    "    return file_path\n",
    "\n",
    "def read_data(file_path):\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return words\n",
    "\n",
    "def build_vocab(words, vocab_size):\n",
    "    dictionary = dict()\n",
    "    count = [(\"UNK\", -1)]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    index = 0\n",
    "    with open(\"processed/vocab_1000.tsv\", \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if (index < 1000):\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_words_to_index(words, dictionary):\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "    \n",
    "def build_cooccurrence_matrix(dictionary, index_words, context_window_size):\n",
    "    n = len(dictionary)\n",
    "    mat = np.zeros((n, n))\n",
    "    for center in index_words:\n",
    "        for context in range(1, context_window_size+1):\n",
    "            # process targets before the center word\n",
    "            target = index_words[max(0, center - context)]\n",
    "            mat[center, target] += 1\n",
    "            # process targets after the center word\n",
    "            target = index_words[min(center + context, len(index_words))]\n",
    "            mat[center, target] += 1\n",
    "    return mat\n",
    "\n",
    "def process_data(vocab_size, skip_window):\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    dictionary, index_dictionary = build_vocab(words, vocab_size)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words # to save memory\n",
    "    print(index_words[:25])\n",
    "    mat = build_cooccurrence_matrix(dictionary, index_words, 2)\n",
    "    print(np.shape(mat))\n",
    "    return mat\n",
    "\n",
    "startTime = datetime.now()\n",
    "mat = process_data(VOCAB_SIZE, SKIP_WINDOW)\n",
    "print(np.shape(mat))\n",
    "print(\"Processing data and constructing matrix took: {0}\".format(datetime.now() - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1000 x 1000\n"
     ]
    }
   ],
   "source": [
    "def context_counting_model(mat, vocab_size, embed_size):\n",
    "    print(\"1\")\n",
    "    cooccurrence_mat = tf.placeholder(tf.float32, shape = [vocab_size,\n",
    "                                                          vocab_size],\n",
    "                                     name = 'cooccurrence_matrix')\n",
    "    print(\"2\")\n",
    "    s, u, v = tf.svd(cooccurrence_mat)\n",
    "    print(\"3\")\n",
    "    smaller_mat = tf.matmul(u[:embed_size], tf.diag(s), b_is_sparse=True)\n",
    "    print(\"4\")\n",
    "    another_mat = tf.matmul(smaller_mat, v[:, :embed_size])\n",
    "    print(\"5\")\n",
    "    print(\"{0} x {0}\".format(vocab_size, vocab_size))\n",
    "    startTime = datetime.now()\n",
    "    with tf.Session() as sess:\n",
    "        another_mat.eval(feed_dict = {cooccurrence_mat: mat})\n",
    "    print(\"Taking the svd took: {0}\".format(datetime.now() - startTime))\n",
    "    \n",
    "context_counting_model(mat, VOCAB_SIZE, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Warning: I debugged for hours and the issue was with Jupyter Notebook, not my Tensorflow model. Lesson: Don't run Tensorflow models in Jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
