{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 Structure Your TensorFlow Model\n",
    "<a href=\"http://web.stanford.edu/class/cs20si/lectures/notes_04.pdf\">CS20 SI Lecture 4 Notes</a><br>\n",
    "<a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">McCormickML Word2vec Tutorial</a>\n",
    "\n",
    "## Don't run the entire notebook! Just run one of the two models, or you'll get errors\n",
    "\n",
    "In this lecture, we will try to build word2vec, a skip-gram model.\n",
    "\n",
    "Skip-gram vs CBOW   (Continuous Bag-of-Words)\n",
    "Algorithmically, these models are similar, except that CBOW predicts center words from context words, while the skip-gram does the inverse and predicts source context-words from the center words. For  example, if we have the sentence: \"The  quick brown fox jumps\", then CBOW tries to predict \"brown\" from \"the\", \"quick\", \"fox\", and \"jumps\", while skip-gram tries to predict \"the\", \"quick\", \"fox\", and \"jumps\" from \"brown\".\n",
    "\n",
    "Statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" style=\"width:500px;height:300px;\"></img>\n",
    "\n",
    "We want 300 features, so 300 neurons. You can see that our 10000 rows become our word vectors (of length 300!)\n",
    "\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" style=\"width:500px;height:300px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CS 224N, we learned about two training methods: hierachical softmax and negative sampling. We rule out softmax because the normalization factor is too computationally expensive and the students in CS 224N implemented the skip-gram model with negative sampling.\n",
    "\n",
    "Negative samplying belongs to a family of sampling-based approachs that also includes importance sampling and target sampling. Negative sampling is a simplified model of Noise Constrastive Estimatino (NCE), e.g. negative sampling makes certain assumptions about the number of noise samples to generate (k) and the distribution of noise sames (Q) (negative sampling assumes that kQ(w) = 1) to simplify computation.\n",
    "\n",
    "While negative sampling is useful for learning word embeddings, it doesn't have the theoretical guarantee that its derivative tends towards the gradient of the softmax function, which makes it not so useful for language modelling. \n",
    "\n",
    "NCE has nice theoretical guarantees that negative sampling lacks as the number of noise samples increases. Mnih and Teh (2012) reported that 25 noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about 45.\n",
    "\n",
    "In this example, we will be using NCE because of its nice theoretical guarantee. Note that sampling-based approaches are only useful at training time - during inference, the full softmax still needs to be computed to obtain a normalized probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** About the dataset **\n",
    "text8 is the first 100 MB of cleaned text of the English Wikipedia dump on Mar. 3, 2006. It is not enough to train really good word embeddings, for better results use the dataset fil9 of the first 10^9 bytes of the Wikipedia dump, as described on Matt Mahoney's website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Interface: How to structure your TensorFlow model **\n",
    "We've only done 2 models in the past, and they more or less have the same structure:\n",
    "\n",
    "Phase 1: assemble your graph\n",
    "1. Define placeholders for input and output\n",
    "2. Define the weights (variable)\n",
    "3. Define the inference model\n",
    "4. Define loss function\n",
    "5. Define optimizer\n",
    "\n",
    "Phase 2: execute the computation (training your model)\n",
    "1. Initialize all model variables for the first time.\n",
    "2. Feed in the training data. Might involve randomizing the order of the data samples. \n",
    "3. Execute the inference model on the training data, so it calculates for each training input example the output with the current model parameters.\n",
    "4. Compute the cost\n",
    "5. Adjust the model parameters to minimize/maximize the cost depending on the model.\n",
    "\n",
    "Let's apply these steps to create our word2vec, skip-gram model.\n",
    "\n",
    "## No-frills word2vec skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "Average loss at step 1999: 113.5\n",
      "Average loss at step 3999:  52.8\n",
      "Average loss at step 5999:  33.5\n",
      "Average loss at step 7999:  23.2\n",
      "Average loss at step 9999:  17.8\n",
      "[[ 0.73839045 -0.15143414 -0.03583032 ..., -0.5138132  -0.07501161\n",
      "  -0.14482619]\n",
      " [-0.40334132 -0.46847922 -0.11962534 ..., -0.76152456 -0.21833348\n",
      "  -0.82695729]\n",
      " [ 0.39207357 -0.30086923  0.69663316 ..., -0.35250014  0.16394222\n",
      "  -0.30729359]\n",
      " ..., \n",
      " [ 0.12885094  0.29688597  0.68227816 ...,  0.4535439  -0.28766537\n",
      "  -0.62341571]\n",
      " [ 0.49020457 -0.24589086 -0.58393574 ...,  0.75632381  0.14895558\n",
      "  -0.97532344]\n",
      " [ 0.98762894 -0.62763882  0.14082861 ...,  0.3909626   0.07093954\n",
      "   0.88820386]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from process_data import process_data\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # the context window\n",
    "NUM_SAMPLED = 64    # Number of negative examples to sample.\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 10000\n",
    "SKIP_STEP = 2000 # how many steps to skip before reporting the loss\n",
    "\n",
    "def word2vec(batch_gen):\n",
    "    \"\"\" Build the graph for word2vec model and train it \"\"\"\n",
    "    \n",
    "    # Step 1: Define the placeholders for input and output\n",
    "    # center_words have to be int to work on embedding lookup\n",
    "    # Input: Center word (Using index number instead of one hot, e.g. 234)\n",
    "    # Output: Target word\n",
    "    with tf.name_scope(\"data\"):\n",
    "        center_words = tf.placeholder(tf.float32, shape = [BATCH_SIZE], \n",
    "                                      name = 'center_words')\n",
    "        target_words = tf.placeholder(tf.float32, shape = [BATCH_SIZE, 1], \n",
    "                                      name = 'target_words')\n",
    "\n",
    "    # Step 2: Define weights. In word2vec, it's actually the weights that \n",
    "    # we care about\n",
    "    # If one word is represented with a vector of size EMBED_SIZE, then the\n",
    "    # embedding matrix will have shape [VOCAB_SIZE, EMBED_SIZE]\n",
    "    # Initialized to random uniform -1 to 1\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope(\"embed\"):\n",
    "            embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, \n",
    "                                                          EMBED_SIZE], \n",
    "                                                         -1.0, 1.0), \n",
    "                                       name = 'embed_matrix')\n",
    "\n",
    "        # Step 3: Define the inference\n",
    "        # Our goal is to get the vector representations of words in our dictionary.\n",
    "        # Each row of the embedding matrix corresponds to the vector representation\n",
    "        # of the word at that index. So to get the representation of all the center\n",
    "        # words in the batch, we get the slice of all corresponding rows in the\n",
    "        # embedding matrix. TensorFlow provides a convenient method to do so called\n",
    "        # tf.nn.embedding_lookup()\n",
    "        # This method is really useful when it comes to matrix multiplication with\n",
    "        # one-hot vectors because it saves us from doing a bunch of computation that\n",
    "        # will return 0 anyway.\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            #  embedding_lookup retrieves rows of the first tensor\n",
    "            embed = tf.nn.embedding_lookup(embed_matrix, tf.to_int32(center_words), name='embed')\n",
    "\n",
    "            # Step 4: construct variables for NCE loss\n",
    "            # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "            # nce_weight (vocab size x embed size), intialized to truncated_normal stddev=1.0 / (EMBED_SIZE ** 0.5)\n",
    "            # bias: vocab size, initialized to 0\n",
    "\n",
    "            \"\"\" Define the loss function \"\"\"\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],\n",
    "                                                stddev = 1.0 / math.sqrt(EMBED_SIZE)), \n",
    "                                     name = \"nce_weight\")\n",
    "            nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name = \"nce_bias\")\n",
    "\n",
    "            # define loss function to be NCE loss function\n",
    "            # tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, ...)\n",
    "            # need to get the mean accross the batch\n",
    "            loss = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weight,\n",
    "                                        biases = nce_bias,\n",
    "                                        labels = target_words,\n",
    "                                        inputs = embed,\n",
    "                                        num_sampled = NUM_SAMPLED,\n",
    "                                        num_classes = VOCAB_SIZE),\n",
    "                                 name=\"loss\")\n",
    "\n",
    "        # Step 5: define optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        total_loss = 0.0 # we use this to calculate the average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter(\"graphs\", sess.graph)\n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            batch = batch_gen.__next__()\n",
    "            # create feed_dict, run optimizer, fetch loss_batch\n",
    "            loss_batch, _ = sess.run([loss, optimizer],\n",
    "                                    feed_dict = {center_words: batch[0],\n",
    "                                                target_words: batch[1]})\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "        print(embed_matrix.eval())\n",
    "        writer.close()\n",
    "\n",
    "def main():\n",
    "    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "    word2vec(batch_gen)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to\n",
    "```\n",
    "source activate tensorflow\n",
    "```\n",
    "Use this in terminal for tensorboard\n",
    "``` \n",
    "tensorboard  --logdir = \"graphs\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can visualize the embedding with PCA or t-SNE:\n",
    "\n",
    "t-distributed stochastic neighbor embedding is a ML algorithm for dimensionality reduction develoepd by Geoffrey Hinton and Laurens van der Maatan. It is a nonlinear dim. reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in the scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec with NCE loss and code to visualize the embeddings on TensorBoard\n",
    "<a href = \"https://github.com/chiphuyen/tf-stanford-tutorials/blob/master/examples/04_word2vec_visualize.py\">Word2vec_visualize.py Reference</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/skip-gram-199999\n",
      "Average loss at step 201999:   4.3\n",
      "Average loss at step 203999:   4.3\n",
      "Average loss at step 205999:   4.3\n",
      "Average loss at step 207999:   4.3\n",
      "Average loss at step 209999:   4.3\n",
      "Average loss at step 211999:   4.3\n",
      "Average loss at step 213999:   4.4\n",
      "Average loss at step 215999:   4.2\n",
      "Average loss at step 217999:   4.2\n",
      "Average loss at step 219999:   4.3\n",
      "Average loss at step 221999:   4.3\n",
      "Average loss at step 223999:   4.3\n",
      "Average loss at step 225999:   4.3\n",
      "Average loss at step 227999:   4.3\n",
      "Average loss at step 229999:   4.3\n",
      "Average loss at step 231999:   4.3\n",
      "Average loss at step 233999:   4.3\n",
      "Average loss at step 235999:   4.3\n",
      "Average loss at step 237999:   4.2\n",
      "Average loss at step 239999:   4.2\n",
      "Average loss at step 241999:   4.3\n",
      "Average loss at step 243999:   4.3\n",
      "Average loss at step 245999:   4.3\n",
      "Average loss at step 247999:   4.3\n",
      "Average loss at step 249999:   4.3\n",
      "Average loss at step 251999:   4.3\n",
      "Average loss at step 253999:   4.3\n",
      "Average loss at step 255999:   4.3\n",
      "Average loss at step 257999:   4.3\n",
      "Average loss at step 259999:   4.3\n",
      "Average loss at step 261999:   4.3\n",
      "Average loss at step 263999:   4.2\n",
      "Average loss at step 265999:   4.0\n",
      "Average loss at step 267999:   4.3\n",
      "Average loss at step 269999:   4.3\n",
      "Average loss at step 271999:   4.3\n",
      "Average loss at step 273999:   4.3\n",
      "Average loss at step 275999:   4.3\n",
      "Average loss at step 277999:   4.3\n",
      "Average loss at step 279999:   4.2\n",
      "Average loss at step 281999:   4.3\n",
      "Average loss at step 283999:   4.3\n",
      "Average loss at step 285999:   4.3\n",
      "Average loss at step 287999:   4.3\n",
      "Average loss at step 289999:   4.4\n",
      "Average loss at step 291999:   4.3\n",
      "Average loss at step 293999:   4.3\n",
      "Average loss at step 295999:   4.3\n",
      "Average loss at step 297999:   4.2\n",
      "Average loss at step 299999:   4.3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf\n",
    "\n",
    "from process_data import process_data\n",
    "\n",
    "import math\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1 # the context window\n",
    "NUM_SAMPLED = 64 # Number of negative examples to sample.\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "WEIGHTS_FLD = 'processed/'\n",
    "SKIP_STEP = 2000\n",
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\" Step 1: define the placeholders for input and output \"\"\"\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        \"\"\" Step 2: define weights. In word2vec, it's actually the weights that we care about \"\"\"\n",
    "        # Assemble this part of the graph on the CPU. You can change it to GPU if you have GPU\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"embed\"):\n",
    "                self.embed_matrix = tf.Variable(tf.random_uniform([self.vocab_size, \n",
    "                                                                    self.embed_size], -1.0, 1.0), \n",
    "                                                                    name='embed_matrix')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\" Step 3 + 4: define the model + the loss function \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # Step 3: define the inference\n",
    "                embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "\n",
    "                # Step 4: define loss function\n",
    "                # construct variables for NCE loss\n",
    "                nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                            stddev=1.0 / math.sqrt(self.embed_size)), name='nce_weight')\n",
    "                nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "\n",
    "                # define loss function to be NCE loss function\n",
    "                self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                                    biases=nce_bias, \n",
    "                                                    labels=self.target_words, \n",
    "                                                    inputs=embed, \n",
    "                                                    num_sampled=self.num_sampled, \n",
    "                                                    num_classes=self.vocab_size), name='loss')\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, \n",
    "                                                              global_step=self.global_step)\n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()\n",
    "\n",
    "def train_model(model, batch_gen, num_train_steps, weights_fld):\n",
    "    saver = tf.train.Saver() # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    initial_step = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('improved_graph/lr' + str(LEARNING_RATE), sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = batch_gen.__next__()\n",
    "            feed_dict={model.center_words: centers, model.target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % SKIP_STEP == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                total_loss = 0.0\n",
    "                saver.save(sess, 'checkpoints/', index)\n",
    "        \n",
    "        ####################\n",
    "        # code to visualize the embeddings. uncomment the below to visualize embeddings\n",
    "        final_embed_matrix = sess.run(model.embed_matrix)\n",
    "        \n",
    "        # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "        embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "        sess.run(embedding_var.initializer)\n",
    "\n",
    "        config = projector.ProjectorConfig()\n",
    "        summary_writer = tf.summary.FileWriter('processed')\n",
    "\n",
    "        # add embedding to the config file\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = embedding_var.name\n",
    "        \n",
    "        # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "        embedding.metadata_path = 'processed/vocab_1000.tsv'\n",
    "\n",
    "        # saves a configuration file that TensorBoard will read during startup.\n",
    "        projector.visualize_embeddings(summary_writer, config)\n",
    "        saver_embed = tf.train.Saver([embedding_var])\n",
    "        saver_embed.save(sess, 'processed/model3.ckpt', 1)\n",
    "\n",
    "def main():\n",
    "    model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build_graph()\n",
    "    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n",
    "    train_model(model, batch_gen, NUM_TRAIN_STEPS, WEIGHTS_FLD)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "source activate tensorflow\n",
    "tensorboard --logdir=\"improved_graph/lr1.0\"\n",
    "tensorboard --logdir=\"checkpoints/\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
