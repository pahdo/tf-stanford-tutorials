{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Regression in TensorFlow **\n",
    "\n",
    "**Problem**: Are fire and theft redundant features? Is there a relationship between the number of fires and the number of thefts?\n",
    "\n",
    "X = number of fires per 1000 housing units\n",
    "\n",
    "Y = number of thefts per 1000 population\n",
    "\n",
    "Within the same Zip code in the Chicago metro area\n",
    "\n",
    "Total number of Zip code areas: 42\n",
    "\n",
    "Find Y = f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2069.6319333978354\n",
      "Epoch 1: 2117.0123581953535\n",
      "Epoch 2: 2092.302723001866\n",
      "Epoch 3: 2068.5080461938464\n",
      "Epoch 4: 2045.591184088162\n",
      "Epoch 5: 2023.5146448101316\n",
      "Epoch 6: 2002.2447619835536\n",
      "Epoch 7: 1981.748338803649\n",
      "Epoch 8: 1961.9944411260742\n",
      "Epoch 9: 1942.9520116143283\n",
      "Epoch 10: 1924.5930823644712\n",
      "Epoch 11: 1906.8898800636332\n",
      "Epoch 12: 1889.8164505837929\n",
      "Epoch 13: 1873.347133841543\n",
      "Epoch 14: 1857.4588400604468\n",
      "Epoch 15: 1842.1278742424079\n",
      "Epoch 16: 1827.332495119955\n",
      "Epoch 17: 1813.0520579712022\n",
      "Epoch 18: 1799.2660847636982\n",
      "Epoch 19: 1785.9562132299961\n",
      "Epoch 20: 1773.1024853109072\n",
      "Epoch 21: 1760.689129482884\n",
      "Epoch 22: 1748.6984157081515\n",
      "Epoch 23: 1737.1138680398553\n",
      "Epoch 24: 1725.920873066732\n",
      "Epoch 25: 1715.1046249579008\n",
      "Epoch 26: 1704.6500954309377\n",
      "Epoch 27: 1694.5447134910141\n",
      "Epoch 28: 1684.7746311347667\n",
      "Epoch 29: 1675.328450968245\n",
      "Epoch 30: 1666.1935385839038\n",
      "Epoch 31: 1657.3584002084322\n",
      "Epoch 32: 1648.8122658529207\n",
      "Epoch 33: 1640.5440742547091\n",
      "Epoch 34: 1632.5446836102221\n",
      "Epoch 35: 1624.8043315147183\n",
      "Epoch 36: 1617.3126799958602\n",
      "Epoch 37: 1610.0622532456405\n",
      "Epoch 38: 1603.0433557207386\n",
      "Epoch 39: 1596.2479176106197\n",
      "Epoch 40: 1589.668056331575\n",
      "Epoch 41: 1583.2965242617897\n",
      "Epoch 42: 1577.126371285745\n",
      "Epoch 43: 1571.1501190634\n",
      "Epoch 44: 1565.360979151513\n",
      "Epoch 45: 1559.7523780798629\n",
      "Epoch 46: 1554.3184364555138\n",
      "Epoch 47: 1549.0529469620615\n",
      "Epoch 48: 1543.950059985476\n",
      "Epoch 49: 1539.0050282141283\n",
      "Epoch 50: 1534.211797797609\n",
      "Epoch 51: 1529.56534988646\n",
      "Epoch 52: 1525.0607591186251\n",
      "Epoch 53: 1520.6934648507852\n",
      "Epoch 54: 1516.4585935090713\n",
      "Epoch 55: 1512.3524023861364\n",
      "Epoch 56: 1508.3695780125756\n",
      "Epoch 57: 1504.5066588066873\n",
      "Epoch 58: 1500.7606269073274\n",
      "Epoch 59: 1497.126336559476\n",
      "Epoch 60: 1493.600210891061\n",
      "Epoch 61: 1490.1794991287668\n",
      "Epoch 62: 1486.8605145300749\n",
      "Epoch 63: 1483.639419928193\n",
      "Epoch 64: 1480.5144186365596\n",
      "Epoch 65: 1477.4811065652452\n",
      "Epoch 66: 1474.5376660533782\n",
      "Epoch 67: 1471.6799176652871\n",
      "Epoch 68: 1468.9063155567717\n",
      "Epoch 69: 1466.2136880280007\n",
      "Epoch 70: 1463.5996563179153\n",
      "Epoch 71: 1461.0614086978492\n",
      "Epoch 72: 1458.597208841216\n",
      "Epoch 73: 1456.2043069711044\n",
      "Epoch 74: 1453.8807724802089\n",
      "Epoch 75: 1451.6242183893032\n",
      "Epoch 76: 1449.432753210976\n",
      "Epoch 77: 1447.3042320180018\n",
      "Epoch 78: 1445.237068621615\n",
      "Epoch 79: 1443.228872676177\n",
      "Epoch 80: 1441.2782130186733\n",
      "Epoch 81: 1439.3831422174615\n",
      "Epoch 82: 1437.542224922173\n",
      "Epoch 83: 1435.7540219968096\n",
      "Epoch 84: 1434.0160684508405\n",
      "Epoch 85: 1432.3276573866606\n",
      "Epoch 86: 1430.687153330871\n",
      "Epoch 87: 1429.093016880254\n",
      "Epoch 88: 1427.543719962062\n",
      "Epoch 89: 1426.038033108981\n",
      "Epoch 90: 1424.5748210840281\n",
      "Epoch 91: 1423.1531702368743\n",
      "Epoch 92: 1421.771026852585\n",
      "Epoch 93: 1420.4274983895677\n",
      "Epoch 94: 1419.121967994741\n",
      "Epoch 95: 1417.85251878131\n",
      "Epoch 96: 1416.618930517208\n",
      "Epoch 97: 1415.4196022436731\n",
      "Epoch 98: 1414.2534379121803\n",
      "Epoch 99: 1413.1202843011845\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhJyCXkQk5P4/tAaTjAGUVWsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111b86d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Assume linearity. Y = wX + b. Let's use mean squared error as the loss function\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Simple linear regression example in TensorFlow\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xlrd\n",
    "\n",
    "DATA_FILE = \"data/fire_theft.xls\"\n",
    "\n",
    "# Step 1: read in the data from the .xls file\n",
    "book = xlrd.open_workbook(DATA_FILE, encoding_override=\"utf-8\")\n",
    "sheet = book.sheet_by_index(0)\n",
    "# data: [[ 6.2 29. ] \\n [ 9.5 44. ] \\n ... ]\n",
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])\n",
    "n_samples = sheet.nrows - 1\n",
    "\n",
    "# Step 2: create placeholders for input X and label Y\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(0.0, name = \"weights\")\n",
    "b = tf.Variable(0.0, name = \"bias\")\n",
    "\n",
    "# Step 4: construct model to predict Y from the number of fires\n",
    "Y_predicted = X * w + b\n",
    "\n",
    "# Step 5: use the square error as the loss function\n",
    "loss = tf.square(Y - Y_predicted, name=\"loss\")\n",
    "\n",
    "# Step 6: use gradient descent with a learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('graphs/03/linear_reg')\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        total_loss = 0\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            _, l = sess.run([optimizer, loss], feed_dict = {X: x, Y: y})\n",
    "            total_loss += l\n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "            \n",
    "    writer.close()\n",
    "    \n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w,b])\n",
    "\n",
    "X, Y = data.T[0], data.T[1]\n",
    "plt.plot(X, Y, 'bo', label='Real data')\n",
    "plt.plot(X, X * w_value + b_value, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code\n",
    "```\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "```\n",
    "is not straightforward -\n",
    "\n",
    "1 Why is train_op in the fetches list of tf.Session.run()\n",
    "\n",
    "* We can actually pass any TensorFlow ops as fetches in tf.Session.run().\n",
    "\n",
    "2 How does TensorFlow know what variables to update?\n",
    "* TensorFlow will execute the part of the graph that those ops depend on. In this case, we see that train_op has the purpose of minimize loss, and loss depends on variables w and b.\n",
    "* By default, the optimizer trains all the trainable variables that the objective functions depends on. If you do not want to train a variable, set trainable=False when you declare a variable. One example of a variable you don't want to train is global_step, a variable to keep track of how many times you've run your model.**\n",
    "\n",
    "```\n",
    "global_step = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "learning_rate = 0.01 * 0.99 ** tf.cast(global_step, tf.float32)\n",
    "\n",
    "increment step = global_step.assign_add(1)\n",
    "optimizer = tf.GradientDescentOptimizer(learning_rate) # learning rate can be a tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask optimizer to take gradients of specific variables \n",
    "and also modify the gradients computed by your optimizer\n",
    "\n",
    "```\n",
    "#create an optimizer.\n",
    "optimizer = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# compute the gradients for a list of variables\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable). Do what you \n",
    "# need to the 'gradient' part, for example, subtract each of them by 1.\n",
    "subtracted_grads_and_vars = [(gv[0] - 1.0, gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# ask the optimizer to apply the subtracted gradients.\n",
    "optimizer.apply_gradients(subtracted_grads_and_vars)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on computing gradients**\n",
    "\n",
    "The optimizer classes automatically compute derivs. on your graph, but creators of new optimizers can call the lower-level functions below.\n",
    "\n",
    "```\n",
    "tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)\n",
    "```\n",
    "\n",
    "This method constructs symbolic partial derivatives of the sum of y's w.r.t. x in x's. y's and x's are each a Tensor or list of tensors. grad_y's is a list of Tensors, holding the gradients received by the ys. The list must be the same length as the y's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports many optimizers...\n",
    "TL;DR: Use AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression in TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built-in function to load MNIST data to the folder data/mnist\n",
    "MNIST = input_data.read_data_sets(\"data/mnist\", one_hot=True)\n",
    "# One hot encoding is like 0001000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.2901001055757482\n",
      "Average loss epoch 1: 0.7330171893804501\n",
      "Average loss epoch 2: 0.6002440593598328\n",
      "Average loss epoch 3: 0.5367404280842601\n",
      "Average loss epoch 4: 0.4981336754101973\n",
      "Average loss epoch 5: 0.47088863042406826\n",
      "Average loss epoch 6: 0.4510021991940923\n",
      "Average loss epoch 7: 0.43620179562301903\n",
      "Average loss epoch 8: 0.42412465664890264\n",
      "Average loss epoch 9: 0.4131479959387879\n",
      "Average loss epoch 10: 0.4043766434514995\n",
      "Average loss epoch 11: 0.39604608770155963\n",
      "Average loss epoch 12: 0.39035088032275644\n",
      "Average loss epoch 13: 0.3845526687193028\n",
      "Average loss epoch 14: 0.37929321241962327\n",
      "Average loss epoch 15: 0.37463911277152995\n",
      "Average loss epoch 16: 0.37144154278965263\n",
      "Average loss epoch 17: 0.36628322879890185\n",
      "Average loss epoch 18: 0.3622230695330457\n",
      "Average loss epoch 19: 0.36032989503064633\n",
      "Average loss epoch 20: 0.35510626911144433\n",
      "Average loss epoch 21: 0.3551364885274069\n",
      "Average loss epoch 22: 0.3516883894210651\n",
      "Average loss epoch 23: 0.34910756113368036\n",
      "Average loss epoch 24: 0.34615395827726886\n",
      "Total time: 15.232210159301758 seconds\n",
      "Optimization finished\n",
      "Accuracy 0.9096\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 2: Define parameters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "\n",
    "# Step 3: Create placeholders for features and labels\n",
    "# Each image in the MNIST data is 28*28 = 784\n",
    "# Therefore, each image is represented by a 1x784 tensor\n",
    "# There are 10 classes for each image, corresponding to 0-9\n",
    "# Each label is a one-hot vector\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 4: Create weights and bias\n",
    "# w is intialized to random variables with mean of 0 and stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# The shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# The shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape = [784, 10], stddev = 0.01), name = \"weights\")\n",
    "b = tf.Variable(tf.zeros([1, 10]), name = \"bias\")\n",
    "\n",
    "# Step 5: Predict Y from X and w, b\n",
    "# The model returns the probability distribution of possibles labels of the image\n",
    "# through the softmax layer: a batch_size x 10 tensor that represents the probabilities\n",
    "# of the digits.\n",
    "logits = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 6: Define loss function\n",
    "# Use softmax cross-entropy with logits as the loss function\n",
    "# Compute mean cross-entropy, softmax is applied internally\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # Computes the mean over examples in the batch\n",
    "\n",
    "# Step 7: Define training op\n",
    "# using gradient descent with learn rate = 0.01 to minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('graphs/03/logistic_reg')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # Train the model n_epochs times\n",
    "        total_loss = 0\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict = {X: X_batch, Y:Y_batch})\n",
    "            total_loss += loss_batch\n",
    "        print(\"Average loss epoch {0}: {1}\".format(i, total_loss/n_batches))\n",
    "        \n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    print(\"Optimization finished\")\n",
    "    \n",
    "    # Test the model\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        # We are calling the optimizer on the test data, so we overfit the test data here...\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict = {X: X_batch, Y: Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        # Similar to numpy.count_nonzero(boolarray)\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/MNIST.test.num_examples))\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "# Average loss should be around 0.35 after 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
